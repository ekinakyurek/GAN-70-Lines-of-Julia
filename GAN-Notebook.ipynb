{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN-70-Lines-of-Julia  \n",
    "This notebook is a demonstration of how to train simple GAN for MNIST by using Julia. It will highlight how one can write a model and related mathematical equations in Julia just like writing a paper. Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We will use Knet.jl deep learning package to compute gradients for the networks and to use GPU arrays if a gpu device available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array{Float32,N} where N"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet,Images, Colors,ImageView\n",
    "include(Pkg.dir(\"Knet\",\"data\",\"mnist.jl\")) #importing MNIST data loader functions\n",
    "global atype = gpu()>=0 ? KnetArray{Float32} : Array{Float32}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model  \n",
    "We will define a generic multi-layer perceptron to use for discriminator and generator. $\\mathbf{w}$ is an array keeps model paramaters and $\\mathbf{x}$ will be input. Keyword arguments $\\mathbf{p}$ is dropout probability, `activation`  is the activation function used in the hidden layers, `outputactivation` is the activation function used in output layer. We also defined `leakyrelu` activation function since it is not defined in default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "leakyrelu (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mlp(w,x;p=0.0,activation=leakyrelu,outputactivation=sigm)\n",
    "    for i=1:2:length(w)\n",
    "        x = w[i]*dropout(mat(x),p) .+ w[i+1]   # mat() used for flatten images to vector.\n",
    "        i<length(w)-1 && (x = activation.(x)) \n",
    "    end\n",
    "    return outputactivation.(x) #output layer\n",
    "end\n",
    "leakyrelu(x;Î±=Float32(0.2)) = max(0,x) + Î±*min(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator and Generator Networks\n",
    "\n",
    "Discriminator and Generator networks are simple MLPs named as `D` and `G` in the code. Loss functions `Jd` and `ğ‘±g` are defined according to equation X in GAN paper. Sample noise function `ğ’©` is a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ğ’© (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global const ğœ€=Float32(1e-8)\n",
    "D(w,x;p=0.0) = mlp(w,x;p=p)\n",
    "G(w,z;p=0.0) = mlp(w,z;p=p) \n",
    "ğ‘±d(ğ—ªd,x,Gz) = -mean(log.(D(ğ—ªd,x)+ğœ€)+log.(1-D(ğ—ªd,Gz)+ğœ€))/2   \n",
    "ğ‘±g(ğ—ªg, ğ—ªd, z) = -mean(log.(D(ğ—ªd,G(ğ—ªg,z))+ğœ€))           \n",
    "ğ’©(input, batch) = atype(randn(Float32, input, batch))  #SampleNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Functions\n",
    "\n",
    "For backpropogation we need the derivatives of loss functions according to model parameters. This is where Knet comes to play. The `Knet.grad` function calculates gradient according to first parameter of any function. So,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "âˆ‡d  = grad(ğ‘±d) # Discriminator gradient\n",
    "âˆ‡g  = grad(ğ‘±g) # Generator gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Initialization\n",
    "It is a generic weight initialization function for MLPs. For each layer, it creates a weight matrix and bias vector, then add them to $W$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initweights (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initweights(hidden,input, output)\n",
    "    ğ—ª = Any[];\n",
    "    x = input\n",
    "    for h in [hidden... output]\n",
    "        push!(ğ—ª, atype(xavier(h,x)), atype(zeros(h, 1))) #FC Layers weights and bias\n",
    "        x = h\n",
    "    end\n",
    "    return ğ—ª  #return model params\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Display\n",
    "\n",
    "This function generates random `number` images and displays here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_and_show (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_and_show(ğ—ª,number,ğ—;fldr=\"generations/\")\n",
    "    Gz = G(ğ—ª[1],ğ’©(ğ—[:ginp],number)) .> 0.5\n",
    "    Gz = permutedims(reshape(Gz,(28,28,number)),(2,1,3))\n",
    "    [display(Gray.(Gz[:,:,i])) for i=1:number]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Function\n",
    "\n",
    "This `runmodel` function is implementing training procedure described in GAN paper. It first update discriminator with specified optimizer, then update generator network. Same function can be used in test mode by passing `train` argument as false. In the test mode it calculates losses instead of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runmodel (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function runmodel(ğ—ª, data, ğ—;dtst=nothing,optim=nothing,train=false,saveinterval=10)\n",
    "    gloss=dloss=total=0.0;\n",
    "    B = ğ—[:batchsize]\n",
    "    for i=1:(train ? ğ—[:epochs]:1)\n",
    "        for (x,_) in data\n",
    "            Gz = G(ğ—ª[1],ğ’©(ğ—[:ginp],B)) #Generate Fake Images\n",
    "            train ? update!(ğ—ª[2], âˆ‡d(ğ—ª[2],x,Gz), optim[2]) : (dloss += 2B*ğ‘±d(ğ—ª[2],x,Gz))\n",
    "            \n",
    "            z=ğ’©(ğ—[:ginp],2B) #Sample z from Noise\n",
    "            train ? update!(ğ—ª[1], âˆ‡g(ğ—ª[1], ğ—ª[2], z), optim[1]) : (gloss += 2B*ğ‘±g(ğ—ª[1],ğ—ª[2],z))    \n",
    "            \n",
    "            total+=2B\n",
    "        end\n",
    "        train ? runmodel(ğ—ª,dtst,ğ—;train=false) : println((gloss/total,dloss/total)) #Print average losses in each epoch\n",
    "        i % saveinterval == 0 && generate_and_show(ğ—ª,10,ğ—)  # save 10 images\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ— = Dict(:batchsize=>32,:epochs=>75,:ginp=>256,:genh=>[512],:disch=>[512],:optim=>Adam,:lr=>0.002);\n",
    "xtrn,ytrn,xtst,ytst = mnist()\n",
    "global dtrn = minibatch(xtrn, ytrn, ğ—[:batchsize]; xtype=atype)\n",
    "global dtst = minibatch(xtst, ytst, ğ—[:batchsize]; xtype=atype)\n",
    "ğ—ª  = (ğ—ªg,ğ—ªd)   = initweights(ğ—[:genh], ğ—[:ginp], 784), initweights(ğ—[:disch], 784, 1)\n",
    "ğš¶  = (ğš¶pg,ğš¶pd) =  optimizers(ğ—ªg,ğ—[:optim];lr=ğ—[:lr]), optimizers(ğ—ªd,ğ—[:optim];lr=ğ—[:lr])\n",
    "runmodel(ğ—ª, dtst, ğ—;optim=ğš¶, train=false) # initial losses\n",
    "runmodel(ğ—ª, dtrn, ğ—;optim=ğš¶,train=true, dtst=dtst) # training  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
