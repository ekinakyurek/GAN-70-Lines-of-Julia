using Knet,ArgParse,Images
include(Pkg.dir("Knet","data","mnist.jl"))
global atype = gpu()>=0 ? KnetArray{Float32} : Array{Float32}

function main()
    ğ— = Dict(:batchsize=>32,:epochs=>100,:ginp=>256,:genh=>[512],:disch=>[512],:optim=>"Adam(lr=0.0002)");
    xtrn,ytrn,xtst,ytst = mnist()
    global dtrn = minibatch(xtrn, ytrn, ğ—[:batchsize]; xtype=atype)
    global dtst = minibatch(xtst, ytst, ğ—[:batchsize]; xtype=atype)
    ğ—ª  = (ğ—ªg,ğ—ªd)    = initweights(ğ—[:genh], ğ—[:ginp], 784), initweights(ğ—[:disch], 784, 1)
    ğš¶   = (ğš¶pg,ğš¶pd) =  initoptim(ğ—ªg,ğ—[:optim]), initoptim(ğ—ªd,ğ—[:optim])
    runmodel(ğ—ª, dtst, ğ—;optim=ğš¶, train=false) # initial losses
    runmodel(ğ—ª, dtrn, ğ—;optim=ğš¶,train=true, dtst=dtst)  # training
    return ğ—ª,ğš¶,dtrn,dtst,ğ—    # return weights,optimizers,dataset,options
end

#Generate and Save
function generate_and_save(ğ—ª,number,ğ—;fldr="generations/")
    Gz = G(ğ—ª[1],ğ’©(ğ—[:ginp],number)) .> 0.5
    Gz = permutedims(reshape(Gz,(28,28,number)),(2,1,3))
    [save(fldr*string(i)*".png",convert(Array{Gray{N0f8},2},Gz[:,:,i])) for i=1:number]
end

#Train
function runmodel(ğ—ª, data, ğ—;dtst=nothing,optim=nothing,train=false,saveinterval=10)
    gloss=dloss=total=0.0;
    B = ğ—[:batchsize]
    for i=1:(train ? ğ—[:epochs]:1)
        for (x,_) in data
            Gz = G(ğ—ª[1],ğ’©(ğ—[:ginp],B)) #Generate Fake Images
            if train; update!(ğ—ª[2], âˆ‡d(ğ—ª[2],x,Gz), optim[2]) #if train update discriminator
            else;     dloss += 2B*Dloss(ğ—ª[2],x,Gz); end      #else calculate loss
            z=ğ’©(ğ—[:ginp],2B) #Sample z from Noise
            if train; update!(ğ—ª[1], âˆ‡g(ğ—ª[1], ğ—ª[2], z), optim[1]) #if train update generator
            else;    gloss += 2B*Gloss(ğ—ª[1],ğ—ª[2],z); end         #else calculate loss
            total+=2B
        end
        train ? runmodel(ğ—ª,dtst,ğ—;train=false):println((gloss/total,dloss/total)) #Print average losses in each epoch
        i % saveinterval == 0 && generate_and_save(ğ—ª,10,ğ—)  # save 10 images
    end
end

#Regular  MLP
function  mlp(w,x;p=0.0,activation=leakyrelu,outputactivation=sigm)
    for i=1:2:length(w)
        x = w[i]*dropout(mat(x),p) .+ w[i+1]  #FC Layer
        i<length(w)-1 && (x = activation.(x)) #Activation
    end
    outputactivation.(x) #Output
end

#Discriminator and Generators
D(w,x;p=0.0) = mlp(w,x;p=p)                                   #Discriminator
ğ’©(input, batch) = convert(atype,randn(Float32, input, batch)) #SampleNoise
G(w,z;p=0.0) = mlp(w,z;p=p)                                   #Generator

#Initialize Weights
function initweights(hidden,input, output)
    ğ—ª = Any[];
    x = input
    for h in [hidden... output]
        push!(ğ—ª, convert(atype, xavier(h,x)), convert(atype, zeros(h, 1))) #FC Layers weights and bias
        x = h
    end
    return ğ—ª  #return model params
end

#Loss Functions and Gradients
global ğœ€=1e-8
Dloss(ğ—ª,x,Gz) = -mean(log.(D(ğ—ª,x)+ğœ€)+log.(1-D(ğ—ª,Gz)+ğœ€))/2 #Discriminator Loss
âˆ‡d = grad(Dloss) #Gradient according to discriminator loss
Gloss(ğ—ªg, ğ—ªd, z) = -mean(log.(ğœ€+D(ğ—ªd,G(ğ—ªg,z)))) #Generator Loss
âˆ‡g  = grad(Gloss) #Gradient according to generator loss

#Extensions
leakyrelu(x;Î±=0.2) = max(0,x) + Î±*min(0,x)                    #LeakyRelu activation
initoptim{T<:Number}(::KnetArray{T},otype)=eval(parse(otype)) #Optimizer initializations for KnetArray
initoptim{T<:Number}(::Array{T},otype)=eval(parse(otype))     #Optimizer initializations for Array
initoptim(a,otype)=map(x->initoptim(x,otype), a)
main() #RUN
