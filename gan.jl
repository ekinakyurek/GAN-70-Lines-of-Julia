using Pkg; for p in ("Knet","Colors","Images"); haskey(Pkg.installed(),p) || Pkg.add(p); end
using Knet, Colors, Images, Statistics
include(Pkg.dir("Knet","data","mnist.jl")) #MNIST data loader functions
global atype = gpu() >= 0 ? KnetArray{Float32} : Array{Float32}

#A generic MLP function with customizable activation functions
function mlp(w,x;p=0.0,activation=elu,outputactivation=sigm)
    for i=1:2:length(w)
        x = w[i]*dropout(mat(x),p) .+ w[i+1] # mat() used for flattening images to a vector.
        i<length(w)-1 && (x = activation.(x)) 
    end
    return outputactivation.(x) #output layer
end

global const ğœ€=Float32(1e-8)
D(w,x;p=0.0) = mlp(w,x;p=p)
G(w,z;p=0.0) = mlp(w,z;p=p) 
ğ‘±d(ğ—ªd,x,Gz) = -mean(log.(D(ğ—ªd,x) .+ ğœ€)+log.((1+ğœ€) .- D(ğ—ªd,Gz)))/2   
ğ‘±g(ğ—ªg, ğ—ªd, z) = -mean(log.(D(ğ—ªd,G(ğ—ªg,z)) .+ ğœ€))           
ğ’©(input, batch) = atype(randn(Float32, input, batch))  #SampleNoise

âˆ‡d  = grad(ğ‘±d) # Discriminator gradient
âˆ‡g  = grad(ğ‘±g) # Generator gradient

function initweights(hidden,input, output)
    ğ—ª = Any[];
    x = input
    for h in [hidden... output]
        push!(ğ—ª, atype(xavier(h,x)), atype(zeros(h, 1))) #FC Layers weights and bias
        x = h
    end
    return ğ—ª  #return model params
end

function generate_and_save(ğ—ª,number,ğ—;fldr="generations/")
    Gz = G(ğ—ª[1], ğ’©(ğ—[:ginp], number)) .> 0.5
    Gz = permutedims(reshape(Gz,(28,28,number)), (2,1,3))
    [save(fldr*string(i)*".png",Gray.(Gz[:,:,i])) for i=1:number]
end

#(if) train ? it updates model parameters : (else) it print losses
function runmodel(ğ—ª, data, ğ—; dtst=nothing, optim=nothing, train=false, saveinterval=20)
    gloss = dloss = total=0.0;
    B = ğ—[:batchsize]
    for i=1:(train ? ğ—[:epochs] : 1)
        for (x,_) in data
            total+=2B
            Gz = G(ğ—ª[1], ğ’©(ğ—[:ginp], B)) #Generate Fake Images
            train ? update!(ğ—ª[2], âˆ‡d(ğ—ª[2],x,Gz), optim[2]) : (dloss += 2B*ğ‘±d(ğ—ª[2], x, Gz))
            
            z=ğ’©(ğ—[:ginp],2B) #Sample z from Noise
            train ? update!(ğ—ª[1], âˆ‡g(ğ—ª[1], ğ—ª[2], z), optim[1]) : (gloss += 2B*ğ‘±g(ğ—ª[1],ğ—ª[2],z))       
        end
        train ? runmodel(ğ—ª, dtst, ğ—; train=false) : println((gloss/total, dloss/total))
        i % saveinterval == 0 && generate_and_save(ğ—ª, 100, ğ—)  # save 10 images
    end
end

function main()
    ğ—=Dict(:batchsize=>32,:epochs=>80,:ginp=>256,:genh=>[512],:disch=>[512],:optim=>Adam,:lr=>0.0002);
    xtrn,ytrn,xtst,ytst = mnist()
    global dtrn = minibatch(xtrn, ytrn, ğ—[:batchsize]; xtype=atype)
    global dtst = minibatch(xtst, ytst, ğ—[:batchsize]; xtype=atype)
    ğ—ª  = (ğ—ªg,ğ—ªd)  = initweights(ğ—[:genh], ğ—[:ginp], 784), initweights(ğ—[:disch], 784, 1)
    ğš¶  = (ğš¶pg,ğš¶pd) = optimizers(ğ—ªg, ğ—[:optim]; lr=ğ—[:lr]), optimizers(ğ—ªd,ğ—[:optim]; lr=ğ—[:lr])
    generate_and_show(ğ—ª,100,ğ—)
    runmodel(ğ—ª, dtst, ğ—; optim=ğš¶, train=false) # initial losses
    runmodel(ğ—ª, dtrn, ğ—; optim=ğš¶, train=true, dtst=dtst) # training 
    ğ—ª,ğš¶,ğ—,(dtrn,dtst)    # return weights,optimizers,options and dataset
end
main() #enjoy!
