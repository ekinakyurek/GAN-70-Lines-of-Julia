using Knet,Images; 
include(Pkg.dir("Knet","data","mnist.jl"))
global atype = gpu()>=0 ? KnetArray{Float32} : Array{Float32}

#A generic MLP function with customizable activation functions 
function mlp(w,x;p=0.0,activation=leakyrelu,outputactivation=sigm)
    for i=1:2:length(w)
        x = w[i]*dropout(mat(x),p) .+ w[i+1]   # mat() used for flatten images to vector.
        i<length(w)-1 && (x = activation.(x)) 
    end
    return outputactivation.(x) #output layer
end
leakyrelu(x;Î±=Float32(0.2)) = max(0,x) + Î±*min(0,x) # LeakyRelu activation

global const ğœ€=Float32(1e-8) #  a small number prevent from getting NaN  in logs
D(w,x;p=0.0) = mlp(w,x;p=p)  #  Discriminator
G(w,z;p=0.0) = mlp(w,z;p=p)  #  Generator
ğ‘±d(ğ—ªd,x,Gz) = -mean(log.(D(ğ—ªd,x)+ğœ€)+log.(1-D(ğ—ªd,Gz)+ğœ€))/2 # Discriminator Loss
ğ‘±g(ğ—ªg, ğ—ªd, z) = -mean(log.(D(ğ—ªd,G(ğ—ªg,z))+ğœ€))             # Generator Loss
ğ’©(input, batch) = atype(randn(Float32, input, batch))      # SampleNoise 

âˆ‡d  = grad(ğ‘±d) # Discriminator gradient
âˆ‡g  = grad(ğ‘±g) # Generator gradient

function initweights(hidden,input, output)
    ğ—ª = Any[];
    x = input
    for h in [hidden... output]
        push!(ğ—ª, atype(xavier(h,x)), atype(zeros(h, 1))) #FC Layers weights and bias
        x = h
    end
    return ğ—ª  #return model params
end

function generate_and_save(ğ—ª,number,ğ—;fldr="generations/")
    Gz = G(ğ—ª[1],ğ’©(ğ—[:ginp],number)) .> 0.5
    Gz = permutedims(reshape(Gz,(28,28,number)),(2,1,3))
    [save(fldr*string(i)*".png",Gray.(Gz[:,:,i])) for i=1:number]
end

#(if) train ? it updates model parameters : (else) it print losses
function runmodel(ğ—ª, data, ğ—;dtst=nothing,optim=nothing,train=false,saveinterval=10)
    gloss=dloss=counter=0.0;
    B = ğ—[:batchsize]
    for i=1:(train ? ğ—[:epochs]:1)
        for (x,_) in data
            counter+=2B
            Gz = G(ğ—ª[1],ğ’©(ğ—[:ginp],B)) #Fake Images
            train ? update!(ğ—ª[2], âˆ‡d(ğ—ª[2],x,Gz), optim[2])      : (dloss += 2B*ğ‘±d(ğ—ª[2],x,Gz)) #update discriminator          
            z=ğ’©(ğ—[:ginp],2B) #Sample z from Noise
            train ? update!(ğ—ª[1], âˆ‡g(ğ—ª[1], ğ—ª[2], z), optim[1]) : (gloss += 2B*ğ‘±g(ğ—ª[1],ğ—ª[2],z)) #update generator               
        end
        train ? runmodel(ğ—ª,dtst,ğ—;train=false) : println((gloss/counter,dloss/counter)) #Print average losses in each epoch
        i % saveinterval == 0 && generate_and_save(ğ—ª,10,ğ—)  # save 10 images to generations folder
    end
end

function main()
    ğ— = Dict(:batchsize=>32,:epochs=>75,:ginp=>256,:genh=>[512],:disch=>[512],:optim=>Adam,:lr=>0.002);
    xtrn,ytrn,xtst,ytst = mnist()
    global dtrn = minibatch(xtrn, ytrn, ğ—[:batchsize]; xtype=atype)
    global dtst = minibatch(xtst, ytst, ğ—[:batchsize]; xtype=atype)
    ğ—ª = (ğ—ªg,ğ—ªd)   = initweights(ğ—[:genh], ğ—[:ginp], 784), initweights(ğ—[:disch], 784, 1)
    ğš¶ = (ğš¶pg,ğš¶pd)  = optimizers(ğ—ªg,ğ—[:optim];lr=ğ—[:lr]), optimizers(ğ—ªd,ğ—[:optim];lr=ğ—[:lr])
    runmodel(ğ—ª, dtst, ğ—;optim=ğš¶, train=false) # initial losses
    runmodel(ğ—ª, dtrn, ğ—;optim=ğš¶,train=true, dtst=dtst)  # training
    ğ—ª,ğš¶,ğ—,(dtrn,dtst)    # return weights,optimizers,options and dataset
end

main() #enjoy!
